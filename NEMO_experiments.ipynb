{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "NEMO_experiments",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aditya-Shandilya1182/coom_experiments/blob/main/NEMO_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nemo_toolkit['all']"
      ],
      "metadata": {
        "id": "sA8e_H3Q7eXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nemo\n",
        "print(nemo.__version__)"
      ],
      "metadata": {
        "id": "ocabwfcE7-na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/NeMo.git"
      ],
      "metadata": {
        "id": "jHrXq1Fw78iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.getcwd())\n",
        "os.chdir('NeMo')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:03:53.926208Z",
          "iopub.execute_input": "2025-07-06T13:03:53.926445Z",
          "iopub.status.idle": "2025-07-06T13:03:53.931337Z",
          "shell.execute_reply.started": "2025-07-06T13:03:53.926421Z",
          "shell.execute_reply": "2025-07-06T13:03:53.930495Z"
        },
        "id": "pu3bwJGZ7YTT",
        "outputId": "e74c9fe5-e13d-48f0-b189-d8ee879504a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "id": "z9Zwk6tU7xhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from nemo.collections.nlp.modules.common.megatron.transformer import ParallelTransformerLayer\n",
        "from nemo.collections.nlp.modules.common.megatron.utils import init_method_normal, scaled_init_method_normal, get_linear_layer\n",
        "from nemo.collections.nlp.modules.common.transformer.transformer_modules import TransformerEmbedding\n",
        "from datasets import load_dataset\n",
        "import tiktoken\n",
        "from tiktoken import get_encoding\n",
        "from omegaconf import OmegaConf"
      ],
      "metadata": {
        "id": "iIr5pjcq708K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/kaggle/working')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:04:29.768738Z",
          "iopub.execute_input": "2025-07-06T13:04:29.769725Z",
          "iopub.status.idle": "2025-07-06T13:04:29.773393Z",
          "shell.execute_reply.started": "2025-07-06T13:04:29.769696Z",
          "shell.execute_reply": "2025-07-06T13:04:29.772747Z"
        },
        "id": "yDJJBmFQ7YTV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(os.getcwd())"
      ],
      "metadata": {
        "id": "QJ6xfY-174K1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NemoGPT(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size=256, num_layers=4, num_heads=4, max_seq_len=128):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = TransformerEmbedding(\n",
        "            hidden_size=hidden_size,\n",
        "            vocab_size=vocab_size,\n",
        "            max_sequence_length=max_seq_len,\n",
        "        )\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            layer_cfg = OmegaConf.create({\n",
        "                \"apply_query_key_layer_scaling\": True,\n",
        "                \"bias_activation_fusion\": False,\n",
        "                \"openai_gelu\": False,\n",
        "                \"onnx_safe\": False,\n",
        "                \"use_cpu_initialization\": False,\n",
        "                \"apply_residual_connection_post_layernorm\": False,\n",
        "                \"precision\": 16,\n",
        "                \"activation\": \"gelu\",\n",
        "                \"normalization\": \"layernorm\",\n",
        "            })\n",
        "\n",
        "            layer = ParallelTransformerLayer(\n",
        "                hidden_size=hidden_size,\n",
        "                ffn_hidden_size=4 * hidden_size,\n",
        "                num_attention_heads=num_heads,\n",
        "                init_method=init_method_normal(0.02),\n",
        "                output_layer_init_method=scaled_init_method_normal(0.02, num_layers),\n",
        "                layer_number=i,\n",
        "                config=layer_cfg,\n",
        "                attention_dropout=0.1,\n",
        "                hidden_dropout=0.1,\n",
        "            )\n",
        "            self.layers.append(layer)\n",
        "\n",
        "        self.ln_f = torch.nn.LayerNorm(hidden_size)\n",
        "        self.lm_head = get_linear_layer(\n",
        "                        hidden_size,\n",
        "                        vocab_size,\n",
        "                        init_method=init_method_normal(0.02)\n",
        "                        )\n",
        "\n",
        "\n",
        "    def forward(self, input_ids, labels=None):\n",
        "        x = self.embedding(input_ids=input_ids)\n",
        "        bsz, seq_len, _ = x.size()\n",
        "        attention_mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).view(1, 1, seq_len, seq_len)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(hidden_states=(x, x), attention_mask=attention_mask)[0]\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = torch.nn.functional.cross_entropy(\n",
        "                logits.view(-1, logits.size(-1)),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return logits, loss\n",
        "\n",
        "        return logits\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:25:53.840706Z",
          "iopub.execute_input": "2025-07-06T13:25:53.841451Z",
          "iopub.status.idle": "2025-07-06T13:25:53.84928Z",
          "shell.execute_reply.started": "2025-07-06T13:25:53.841426Z",
          "shell.execute_reply": "2025-07-06T13:25:53.848519Z"
        },
        "id": "6GAJj-dC7YTW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"roneneldan/TinyStories\")"
      ],
      "metadata": {
        "id": "sjHgXRei7pr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "encoded_train_chunks = []\n",
        "for sample in dataset['train']:\n",
        "    encoded_train_chunks.append(tokenizer.encode(sample['text']))\n",
        "train_encoded = torch.tensor([token for chunk in encoded_train_chunks for token in chunk], dtype=torch.long)\n",
        "encoded_val_chunks = []\n",
        "for sample in dataset['validation']:\n",
        "    encoded_val_chunks.append(tokenizer.encode(sample['text']))\n",
        "val_encoded = torch.tensor([token for chunk in encoded_val_chunks for token in chunk], dtype=torch.long)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:04:47.452158Z",
          "iopub.execute_input": "2025-07-06T13:04:47.452412Z",
          "iopub.status.idle": "2025-07-06T13:12:16.176062Z",
          "shell.execute_reply.started": "2025-07-06T13:04:47.452386Z",
          "shell.execute_reply": "2025-07-06T13:12:16.17547Z"
        },
        "id": "QV5yFA1K7YTZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_encoded))\n",
        "print(len(val_encoded))"
      ],
      "metadata": {
        "id": "LsflplRf7sGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "train_encoded.to(device)\n",
        "val_encoded.to(device)"
      ],
      "metadata": {
        "id": "JiKzA7aw7tvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "model = NemoGPT(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_size=256,\n",
        "        num_layers=4,\n",
        "        num_heads=4,\n",
        "        max_seq_len=128,\n",
        "    )"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:26:05.950393Z",
          "iopub.execute_input": "2025-07-06T13:26:05.951126Z",
          "iopub.status.idle": "2025-07-06T13:26:06.280071Z",
          "shell.execute_reply.started": "2025-07-06T13:26:05.951095Z",
          "shell.execute_reply": "2025-07-06T13:26:06.279274Z"
        },
        "id": "-Djo4rh37YTa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "block_size = 512"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:19:55.046598Z",
          "iopub.execute_input": "2025-07-06T13:19:55.046949Z",
          "iopub.status.idle": "2025-07-06T13:19:55.050506Z",
          "shell.execute_reply.started": "2025-07-06T13:19:55.046927Z",
          "shell.execute_reply": "2025-07-06T13:19:55.049818Z"
        },
        "id": "mkZwsUED7YTa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        data = train_encoded if split == 'train' else val_encoded\n",
        "\n",
        "        if data.size(0) <= block_size:\n",
        "            raise ValueError(f\"{split.capitalize()} dataset size is too small for the requested block size.\")\n",
        "\n",
        "        losses = torch.zeros(eval_iters)\n",
        "\n",
        "        for k in range(eval_iters):\n",
        "            ix = torch.randint(0, data.size(0) - block_size, (batch_size,))\n",
        "            x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "            y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "\n",
        "        out[split] = losses.mean().item()\n",
        "\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-06T13:19:55.892724Z",
          "iopub.execute_input": "2025-07-06T13:19:55.893459Z",
          "iopub.status.idle": "2025-07-06T13:19:55.899595Z",
          "shell.execute_reply.started": "2025-07-06T13:19:55.893432Z",
          "shell.execute_reply": "2025-07-06T13:19:55.898927Z"
        },
        "id": "3ixinPjX7YTb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "max_iters = 100\n",
        "gradient_accumulation_steps = 5\n",
        "eval_iters = 10\n",
        "model = model.to(device)\n",
        "for iter in range(max_iters):\n",
        "    print(iter)\n",
        "    if iter % eval_iters == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
        "\n",
        "    ix = torch.randint(len(train_encoded) - block_size, (batch_size,))\n",
        "    x = torch.stack([train_encoded[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([train_encoded[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    logits, loss = model.forward(x, y)\n",
        "\n",
        "    loss = loss / gradient_accumulation_steps\n",
        "    loss.backward()\n",
        "\n",
        "    if (iter + 1) % gradient_accumulation_steps == 0:\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    if (iter + 1) % gradient_accumulation_steps == 0:\n",
        "        print(f\"Loss at step {iter + 1}: {loss.item() * gradient_accumulation_steps:.3f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vllck-t67YTb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}